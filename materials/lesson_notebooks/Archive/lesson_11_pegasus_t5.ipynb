{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 11 Session Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import transformers\n",
    "\n",
    "from transformers import PegasusTokenizer, TFPegasusModel, TFPegasusForConditionalGeneration\n",
    "from transformers import T5Tokenizer, TFT5Model, TFT5ForConditionalGeneration\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"  # Turn off GPU to avoid Resource Exhaustion issues. Ok for quick demo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers==4.3.2\n"
     ]
    }
   ],
   "source": [
    "# See if transformers library is installed.  If not you will want to un-comment\n",
    "# ...and execute the following line\n",
    "#!pip install transformers\n",
    "!pip freeze | grep transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Huggingface example:\n",
    "\n",
    "ARTICLE_TO_SUMMARIZE = ( \"PG&E stated it scheduled the blackouts in response to forecasts for high winds \\\n",
    "            amid dry conditions. The aim is to reduce the risk of wildfires. Nearly 800 thousand customers were \\\n",
    "            scheduled to be affected by the shutoffs which were expected to last through \\\n",
    "            at least midday tomorrow.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pegasus\n",
    "\n",
    "See: https://huggingface.co/transformers/model_doc/pegasus.html#tfpegasusforconditionalgeneration\n",
    "\n",
    "Let us first check out Pegasus. We start by creating the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFPegasusForConditionalGeneration.\n",
      "\n",
      "All the layers of TFPegasusForConditionalGeneration were initialized from the model checkpoint at google/pegasus-xsum.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFPegasusForConditionalGeneration for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_pegasus_for_conditional_generation\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "model (TFPegasusMainLayer)   multiple                  569748480 \n",
      "=================================================================\n",
      "Total params: 569,844,583\n",
      "Trainable params: 569,748,480\n",
      "Non-trainable params: 96,103\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "pegasus_model = TFPegasusForConditionalGeneration.from_pretrained('google/pegasus-xsum')\n",
    "pegasus_tokenizer = PegasusTokenizer.from_pretrained('google/pegasus-xsum')\n",
    "\n",
    "pegasus_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "570 million parameters. Not tiny for sure.\n",
    "\n",
    "Next we greate the input tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pegasus_inputs = pegasus_tokenizer([ARTICLE_TO_SUMMARIZE], return_tensors='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 55])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pegasus_inputs['input_ids'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we check how Pegasus works and how well it performs. We also test the impact of a couple of generation parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"California's largest utility has cut power to hundreds of thousands of customers in an effort to reduce the risk of wildfires.\"]\n"
     ]
    }
   ],
   "source": [
    "pegasus_summary_ids = pegasus_model.generate(pegasus_inputs['input_ids'])\n",
    "\n",
    "print([pegasus_tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) \n",
    "       for g in pegasus_summary_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"California's largest utility has cut power to hundreds of thousands of customers in an effort to reduce the risk of wildfires.\"]\n"
     ]
    }
   ],
   "source": [
    "pegasus_summary_ids = pegasus_model.generate(pegasus_inputs['input_ids'], \n",
    "                                    min_length=20,\n",
    "                                    max_length=40,\n",
    "                                    early_stopping=True)\n",
    "\n",
    "print([pegasus_tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) \n",
    "       for g in pegasus_summary_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"California's largest utility has cut power to hundreds of thousands of customers in an effort to reduce the risk of wildfires.\"]\n"
     ]
    }
   ],
   "source": [
    "pegasus_summary_ids = pegasus_model.generate(pegasus_inputs['input_ids'], \n",
    "                                    no_repeat_ngram_size=2,\n",
    "                                    min_length=20,\n",
    "                                    max_length=40,\n",
    "                                    early_stopping=True)\n",
    "\n",
    "print([pegasus_tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) \n",
    "       for g in pegasus_summary_ids])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No change so far. What if we force it to go a bit longer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"California's largest electricity provider has turned off power to hundreds of thousands of customers in an effort to reduce the risk of wildfires, the company said in a statement on Monday.\"]\n"
     ]
    }
   ],
   "source": [
    "pegasus_summary_ids = pegasus_model.generate(pegasus_inputs['input_ids'], \n",
    "                                    no_repeat_ngram_size=2,\n",
    "                                    min_length=35,\n",
    "                                    max_length=80,\n",
    "                                    early_stopping=True)\n",
    "\n",
    "print([pegasus_tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) \n",
    "       for g in pegasus_summary_ids])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still good! But...\n",
    "\n",
    "#### ... Question: *where are the tokens  \"California's\", \"company\", \"Monday\" coming from?\"*\n",
    "\n",
    "Interesting. Solid summary - obviously with some external knowledge applied - and beam search had no impact here.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T5\n",
    "\n",
    "see: https://huggingface.co/transformers/model_doc/t5.html#tft5forconditionalgeneration\n",
    "\n",
    "Now we do the same for T5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ab17244f0814d3aaacc8596e4ae98c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "001f863413974da39ab8cee1ec12ae12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the layers of TFT5ForConditionalGeneration were initialized from the model checkpoint at t5-large.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "576e0b3a0021462db7351a773fc6f65b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_t5for_conditional_generation\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "shared (TFSharedEmbeddings)  multiple                  32899072  \n",
      "_________________________________________________________________\n",
      "encoder (TFT5MainLayer)      multiple                  302040576 \n",
      "_________________________________________________________________\n",
      "decoder (TFT5MainLayer)      multiple                  402728448 \n",
      "=================================================================\n",
      "Total params: 737,668,096\n",
      "Trainable params: 737,668,096\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "t5_model = TFT5ForConditionalGeneration.from_pretrained('t5-large')\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained('t5-large')\n",
    "\n",
    "t5_model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bit larger than Pegasus. \n",
    "\n",
    "When we create the input we need to tell T5 what we want it to do! So we need to prepend the input with 'summarize: '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_input_text = \"summarize: \" + ARTICLE_TO_SUMMARIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_inputs = t5_tokenizer([t5_input_text], return_tensors='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And again, we will look at the generated output for the same text and also explore some generation parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PG&E said it scheduled the blackouts in response to forecasts for high winds']\n"
     ]
    }
   ],
   "source": [
    "# Generate Summary\n",
    "t5_summary_ids = t5_model.generate(t5_inputs['input_ids'])\n",
    "\n",
    "print([t5_tokenizer.decode(g, skip_special_tokens=True, \n",
    "                           clean_up_tokenization_spaces=False) for g in t5_summary_ids])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty short... can we force it to do more?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PG&E said it scheduled the blackouts in response to forecasts for high winds amid dry conditions . aim is to reduce the risk of wildfires .']\n"
     ]
    }
   ],
   "source": [
    "# Generate Summary\n",
    "t5_summary_ids = t5_model.generate(t5_inputs['input_ids'], min_length=20,  max_length=40)\n",
    "\n",
    "print([t5_tokenizer.decode(g, skip_special_tokens=True, \n",
    "                           clean_up_tokenization_spaces=False) for g in t5_summary_ids])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, that's longer. Still pretty extractive though. Can we do better with beams?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PG&E said it scheduled the blackouts in response to forecast for high winds amid dry conditions and aim is reduce risk of wildfire.']\n"
     ]
    }
   ],
   "source": [
    "t5_summary_ids = t5_model.generate(t5_inputs['input_ids'],\n",
    "                                    num_beams=3,\n",
    "                                    no_repeat_ngram_size=1,\n",
    "                                    min_length=20,\n",
    "                                    max_length=40)\n",
    "                             \n",
    "print([t5_tokenizer.decode(g, skip_special_tokens=True, \n",
    "                           clean_up_tokenization_spaces=False) for g in t5_summary_ids])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we force it to even go a bit longer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PG&E said it scheduled the blackouts in response to forecast for high winds amid dry conditions and aim is reduce risk of wildfire. nearly 800,000 customers were due be affected by shutoff which was expected last through at least midday tomorrow morning']\n"
     ]
    }
   ],
   "source": [
    "t5_summary_ids = t5_model.generate(t5_inputs['input_ids'],\n",
    "                                    num_beams=3,\n",
    "                                    no_repeat_ngram_size=1,\n",
    "                                    min_length=35,\n",
    "                                    max_length=80)\n",
    "                             \n",
    "print([t5_tokenizer.decode(g, skip_special_tokens=True, \n",
    "                           clean_up_tokenization_spaces=False) for g in t5_summary_ids])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm.. almost back to original now.\n",
    "\n",
    "### Summary\n",
    "\n",
    "Based on these quick tests it appears that Pegasus is doing a better job, but...\n",
    "\n",
    "#### Question: *Why may this be an unfair comparison?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
