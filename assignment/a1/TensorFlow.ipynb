{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Machine Learning in TensorFlow and Keras\n",
        "\n",
        "### Brief Review of Machine Learning\n",
        "\n",
        "In supervised learning, parametric models are those where the model is a function of a fixed form with a number of unknown _parameters_.  Together with a loss function and a training set, an optimizer can select parameters to minimize the loss with respect to the training set.  Common optimizers include stochastic gradient descent.  It tweaks the parameters slightly to move the loss \"downhill\" due to a small batch of examples from the training set.\n",
        "\n",
        "### Linear & Logistic Regression\n",
        "\n",
        "You've likely seen linear regression before.  In linear regression, we fit a line (technically, hyperplane) that predicts a target variable, $y$, based on some features $x$.  The form of this model is affine (even if we call it \"linear\"):  \n",
        "\n",
        "$$y_{hat} = xW + b$$\n",
        "\n",
        "where $W$ and $b$ are weights and an offset, respectively, and are the parameters of this parametric model.  The loss function that the optimizer uses to fit these parameters is the squared error ($||\\cdots||_2$) between the prediction and the ground truth in the training set.\n",
        "\n",
        "You've also likely seen logistic regression, which is tightly related to linear regression.  Logistic regression also fits a line - this time separating the positive and negative examples of a binary classifier.  The form of this model is similar: \n",
        "\n",
        "$$y_{hat} = \\sigma(xW + b)$$\n",
        "\n",
        "where again $W$ and $b$ are the parameters of this model, and $\\sigma$ is the [sigmoid function](https://en.wikipedia.org/wiki/Sigmoid_function) which maps un-normalized scores (\"logits\") to values $\\hat{y} \\in [0,1]$ that represent probabilities. The loss function that the optimizer uses to fit these parameters is the [cross entropy](../../materials/lesson_notebook/lesson_1_NN_Review.ipynb) between the prediction and the ground truth in the training set.\n",
        "\n",
        "This pattern of an affine transform, $xW + b$, occurs over and over in machine learning.\n",
        "\n",
        "### Preliminaries...\n",
        "\n",
        "Before we do anything else, let's load our data and take a quick look at it.  In this example, we're going to build a (very) simple binary classifier based on two floating point features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import data\n",
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "X_train, y_train, X_test, y_test = data.generate_data(2500, 500) #large test size to make diagrams better\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "plt.scatter(X_train[:,0], X_train[:,1], c=y_train, cmap='bwr');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training Logistic Regression\n",
        "\n",
        "It's clear that the data is separable with a vertical line.  The simplest model we can use for this data is logistic regression.  Let's do that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "#####\n",
        "## MAKE SURE YOU UNDERSTAND THIS CODE!!\n",
        "##\n",
        "## Look up keras.Sequential and keras.layers.Dense!\n",
        "##\n",
        "## You will need to use them to write your own model down below!\n",
        "#####\n",
        "\n",
        "# Sequential models are ones where the set of specified layers are stacked each on top of the previous.\n",
        "linear_model = keras.Sequential([\n",
        "    # Dense is an affine (xW + b) layer followed by an element wise nonlinearity.\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# adam optimizer is a fancier version of gradient descent.  You can read more about it here: https://arxiv.org/pdf/1412.6980.pdf\n",
        "linear_model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',  # From information theory notebooks.\n",
        "              metrics=['accuracy'])        # What metric to output as we train.\n",
        "\n",
        "linear_model.fit(X_train, y_train, epochs=15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Hint:** You should expect to see an initial loss here of 0.2 - 1.2.  This is because a well-initialized random classifier tends to output a uniform distribution.  For each example in the batch, we either compute the cross-entropy loss of the label (`[1, 0]` or `[0, 1]`) against the model's output (`~[0.5, 0.5]`).  Both cases result in $-\\lg(0.5) = lg(2) = 1.0$.\n",
        "\n",
        "Of course, your random classifier won't output exactly uniform distributions (it's random after all), but you should anticipate it being pretty close.  If it's not, your initialization may be broken and make it hard for your network to learn.\n",
        "\n",
        "**[Optional]** Some technical details... if your randomly initialized network is outputting very confident predictions, the loss computed may be very large while at the same time the sigmoids in the network are likely in saturation, quickly shrinking gradients.  The result is that you make tiny updates in the face of a huge loss.\n",
        "\n",
        "Let's use our model to make predictions on the test set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "predictions = linear_model.predict(X_test)\n",
        "plt.scatter(X_test[:,0], X_test[:,1], c=predictions[:,0]>0.5, cmap='bwr')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### But what about a harder problem?\n",
        "\n",
        "The case above, the data was linearly separable making it susceptible to a linear classifier.\n",
        "\n",
        "But what if you had data that looked more like this?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, y_train, X_test, y_test = data.generate_non_linear_data(2500, 500)\n",
        "plt.scatter(X_test[:,0], X_test[:,1], c=y_test, cmap='bwr')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "linear_model.fit(X_train, y_train, epochs=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Again, let's make predictions on the test set..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "predictions = linear_model.predict(X_test)\n",
        "plt.scatter(X_test[:,0], X_test[:,1], c=predictions[:,0]>0.5, cmap='bwr')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "That isn't very good!\n",
        "\n",
        "### Building a deeper network with the Sequential API\n",
        "\n",
        "Ok, now it's your turn.  Build a sequential neural network below and start to build intuition around the effects of the number of layers in the network and the number of neurons in each layer.  Try to achieve a loss less than 0.05.  Initialization is random, but try to make it happen \"almost\" always (e.g. at least 90% of the time). Let's look first at the number of layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "deep_model = keras.Sequential([\n",
        "    #\n",
        "    # Hint, try \"relu\" as your activation function.\n",
        "    # relu(z) = max(0, z).\n",
        "    #     Note that: relu(z) = z when z > 0\n",
        "    #                relu(z) = 0 otherwise\n",
        "    #\n",
        "    # See https://en.wikipedia.org/wiki/File:Ramp_function.svg\n",
        "    #\n",
        "    # This is the most common nonlinearity for the main body of the network as its derivative is\n",
        "    # either 0 or 1, depending on the value of z.\n",
        "    #\n",
        "    # This means that the gradient doesn't tend to explode or vanish as you multiply more partial\n",
        "    # derivative terms together.\n",
        "    #\n",
        "    # For this problem...\n",
        "    #\n",
        "    # Try toying with the trade offs between more layers vs wider networks:\n",
        "    #   If we keep repeating the same hidden layer with n neurons: \n",
        "    #    What's the minimum number of hidden layers you can get away with given a larger value of n?\n",
        "    #    For this exercise the value of n should be constant across all your layers and n > 5\n",
        "\n",
        "    ### YOUR CODE HERE\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    ### END YOUR CODE\n",
        "    \n",
        "    # Think about why you still use a sigmoid at the top of your network.\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "deep_model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "deep_model.fit(X_train, y_train, epochs=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "predictions = deep_model.predict(X_test)\n",
        "plt.scatter(X_test[:,0], X_test[:,1], c=predictions[:,0]>0.5, cmap='bwr')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Question 4**: What is the minimum number of hidden layers with the same number of neurons in each you can get away with and still achieve the desired loss on the training set?  Enter your answer in the answers file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "narrow_model = keras.Sequential([\n",
        "    #\n",
        "    # Hint, try \"relu\" as your activation function.\n",
        "    # relu(z) = max(0, z).\n",
        "    #     Note that: relu(z) = z when z > 0\n",
        "    #                relu(z) = 0 otherwise\n",
        "    #\n",
        "    # See https://en.wikipedia.org/wiki/File:Ramp_function.svg\n",
        "    #\n",
        "    # This is the most common nonlinearity for the main body of the network as its derivative is\n",
        "    # either 0 or 1, depending on the value of z.\n",
        "    #\n",
        "    # This means that the gradient doesn't tend to explode or vanish as you multiply more partial\n",
        "    # derivative terms together.\n",
        "    #\n",
        "    # For this problem...\n",
        "    #\n",
        "    # Try toying with the trade offs between more layers vs wider networks:\n",
        "    #   If we keep repeating the same hidden layer with n neurons: \n",
        "    #    What's the smallest number of neurons (n) in each layer you can use \n",
        "    #     if you use four or more layers?  Assume\n",
        "\n",
        "    ### YOUR CODE HERE\n",
        "    \n",
        "    \n",
        "    \n",
        "    ### END YOUR CODE\n",
        "    \n",
        "    # Think about why you still use a sigmoid at the top of your network.\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "narrow_model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "narrow_model.fit(X_train, y_train, epochs=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_predictions = narrow_model.predict(X_test)\n",
        "plt.scatter(X_test[:,0], X_test[:,1], c=n_predictions[:,0]>0.5, cmap='bwr')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Question 5**: What is the smallest number of neurons you can use in a layer in the network with the largest number of layers and still get the desired loss on the training set?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Keras Functional API\n",
        "\n",
        "Finally, let's do some initial exploration of the Keras Functional API. The code above used a Sequential model, which is a simplified way to create a Tensorflow model using Keras. As you can see above, we can build a Sequential model just by listing some layers, which Keras will attach one after the other. If we want to do anything fancier, though, like have multiple inputs or outputs, we need to use the Functional API.\n",
        "\n",
        "Let's imagine that we want to take the input features we used above, but now we want to pass the same input into two separate Dense layers, then concatenate the vectors that come out of them. There isn't a practical reason to do this right now, we're just getting familiar with some more Keras layer operations, that you might use in the future when you do have a reason (e.g. if you have two different types of input or you want to generate multiple different outputs).\n",
        "\n",
        "With the Functional API, we first instantiate each layer, specifying any parameters like the dimension (number of neurons). Then, we \"call\" the layer we just created, using another set of parentheses to pass in whatever the input to that layer is. That's how we connect the layers, we save the output from one layer and then pass it as input to the layer that should come after. Essentially we will build our model by repeating:\n",
        "\n",
        "```layer_output = layer(earlier_layer_output)```\n",
        "\n",
        "The order in which we define the layers does not matter.  Once defined we can connect them.\n",
        "\n",
        "\n",
        "We also have to explicitly define an input layer, so that we can pass it into the first hidden layer in the model. We also have to define an output layer.\n",
        "\n",
        "You will redefine the sequential model you just created but do so using the functional API.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the input layer\n",
        "input_layer = keras.layers.Input(shape=(2,), dtype='int64')\n",
        "\n",
        "\n",
        "#Here are the layers you should use\n",
        "\n",
        "dense_1 = keras.layers.Dense(10, activation='relu')  \n",
        "dense_2 = keras.layers.Dense(10, activation='relu')\n",
        "dense_3 = keras.layers.Dense(10, activation='relu')\n",
        "\n",
        "# Now call the dense layers with the right input for each, to connect the network\n",
        "\n",
        "### YOUR CODE HERE\n",
        "    \n",
        "\n",
        "\n",
        "### END YOUR CODE\n",
        "\n",
        "\n",
        "# Define a binary classification layer like we used in the model above\n",
        "classification_layer = keras.layers.Dense(1, activation='sigmoid')(dense_3_out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You've defined all of your layers and connnected them together. Now you just need a few more lines of code to define the overall model, then compile it and train it like you did with the Sequential model.\n",
        "\n",
        "To define the model, you specify the initial inputs and final outputs. You've already defined how the layers are connected internally, so as long as there's a path from the initial inputs to the final outputs, you're good."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the complete model, specifying the overall inputs and outputs\n",
        "func_model = tf.keras.models.Model(inputs=[input_layer], outputs=[classification_layer])\n",
        "\n",
        "# Compile the model, specifying the loss, optimizer, etc to use in training\n",
        "func_model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "func_model.fit(X_train, y_train, epochs=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Question 6**:  What is the accuracy score you get after training the functional model for 10 epochs?  Please copy and paste your answer in to the answers file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "predictions = func_model.predict(X_test)\n",
        "plt.scatter(X_test[:,0], X_test[:,1], c=predictions[:,0]>0.5, cmap='bwr')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Congratulations...\n",
        "... you've trained a nonlinear classifier with TensorFlow and Keras using the sequential and the functional APIs!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
